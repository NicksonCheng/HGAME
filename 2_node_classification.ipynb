{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Single Machine Multi-GPU Minibatch Node Classification\n",
        "======================================================\n",
        "\n",
        "In this tutorial, you will learn how to use multiple GPUs in training a\n",
        "graph neural network (GNN) for node classification.\n",
        "\n",
        "(Time estimate: 8 minutes)\n",
        "\n",
        "This tutorial assumes that you have read the :doc:`Training GNN with Neighbor\n",
        "Sampling for Node Classification <../large/L1_large_node_classification>`\n",
        "tutorial. It also assumes that you know the basics of training general\n",
        "models with multi-GPU with ``DistributedDataParallel``.\n",
        "\n",
        "<div class=\"alert alert-info\"><h4>Note</h4><p>See `this tutorial <https://pytorch.org/tutorials/intermediate/ddp_tutorial.html>`__\n",
        "   from PyTorch for general multi-GPU training with ``DistributedDataParallel``.  Also,\n",
        "   see the first section of :doc:`the multi-GPU graph classification\n",
        "   tutorial <1_graph_classification>`\n",
        "   for an overview of using ``DistributedDataParallel`` with DGL.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading Dataset\n",
        "---------------\n",
        "\n",
        "OGB already prepared the data as a ``DGLGraph`` object. The following code is\n",
        "copy-pasted from the :doc:`Training GNN with Neighbor Sampling for Node\n",
        "Classification <../large/L1_large_node_classification>`\n",
        "tutorial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dgl\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dgl.nn import SAGEConv\n",
        "from ogb.nodeproppred import DglNodePropPredDataset\n",
        "import tqdm\n",
        "import sklearn.metrics\n",
        "\n",
        "dataset = DglNodePropPredDataset('ogbn-arxiv')\n",
        "\n",
        "graph, node_labels = dataset[0]\n",
        "# Add rev edges since ogbn-arxiv is unidirectional.\n",
        "graph = dgl.add_rev_edges(graph)\n",
        "graph.ndata['label'] = node_labels[:, 0]\n",
        "\n",
        "node_features = graph.ndata['feat']\n",
        "num_features = node_features.shape[1]\n",
        "num_classes = (node_labels.max() + 1).item()\n",
        "\n",
        "idx_split = dataset.get_idx_split()\n",
        "train_nids = idx_split['train']\n",
        "valid_nids = idx_split['valid']\n",
        "test_nids = idx_split['test']    # Test node IDs, not used in the tutorial though."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining Model\n",
        "--------------\n",
        "\n",
        "The model will be again identical to the :doc:`Training GNN with Neighbor\n",
        "Sampling for Node Classification <../large/L1_large_node_classification>`\n",
        "tutorial.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, in_feats, h_feats, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_feats, h_feats, aggregator_type='mean')\n",
        "        self.conv2 = SAGEConv(h_feats, num_classes, aggregator_type='mean')\n",
        "        self.h_feats = h_feats\n",
        "\n",
        "    def forward(self, mfgs, x):\n",
        "        h_dst = x[:mfgs[0].num_dst_nodes()]\n",
        "        h = self.conv1(mfgs[0], (x, h_dst))\n",
        "        h = F.relu(h)\n",
        "        h_dst = h[:mfgs[1].num_dst_nodes()]\n",
        "        h = self.conv2(mfgs[1], (h, h_dst))\n",
        "        return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining Training Procedure\n",
        "---------------------------\n",
        "\n",
        "The training procedure will be slightly different from what you saw\n",
        "previously, in the sense that you will need to\n",
        "\n",
        "* Initialize a distributed training context with ``torch.distributed``.\n",
        "* Wrap your model with ``torch.nn.parallel.DistributedDataParallel``.\n",
        "* Add a ``use_ddp=True`` argument to the DGL dataloader you wish to run\n",
        "  together with DDP.\n",
        "\n",
        "You will also need to wrap the training loop inside a function so that\n",
        "you can spawn subprocesses to run it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run(proc_id, devices):\n",
        "    # Initialize distributed training context.\n",
        "    print(proc_id)\n",
        "    print(devices)\n",
        "    dev_id = devices[proc_id]\n",
        "    dist_init_method = 'tcp://{master_ip}:{master_port}'.format(master_ip='127.0.0.1', master_port='12345')\n",
        "    if torch.cuda.device_count() < 1:\n",
        "        device = torch.device('cpu')\n",
        "        torch.distributed.init_process_group(\n",
        "            backend='gloo', init_method=dist_init_method, world_size=len(devices), rank=proc_id)\n",
        "    else:\n",
        "        torch.cuda.set_device(dev_id)\n",
        "        device = torch.device('cuda:' + str(dev_id))\n",
        "        torch.distributed.init_process_group(\n",
        "            backend='nccl', init_method=dist_init_method, world_size=len(devices), rank=proc_id)\n",
        "    \n",
        "    # Define training and validation dataloader, copied from the previous tutorial\n",
        "    # but with one line of difference: use_ddp to enable distributed data parallel\n",
        "    # data loading.\n",
        "    sampler = dgl.dataloading.NeighborSampler([4, 4])\n",
        "    train_dataloader = dgl.dataloading.DataLoader(\n",
        "        # The following arguments are specific to NodeDataLoader.\n",
        "        graph,              # The graph\n",
        "        train_nids,         # The node IDs to iterate over in minibatches\n",
        "        sampler,            # The neighbor sampler\n",
        "        device=device,      # Put the sampled MFGs on CPU or GPU\n",
        "        use_ddp=True,       # Make it work with distributed data parallel\n",
        "        # The following arguments are inherited from PyTorch DataLoader.\n",
        "        batch_size=1024,    # Per-device batch size.\n",
        "                            # The effective batch size is this number times the number of GPUs.\n",
        "        shuffle=True,       # Whether to shuffle the nodes for every epoch\n",
        "        drop_last=False,    # Whether to drop the last incomplete batch\n",
        "        num_workers=0       # Number of sampler processes\n",
        "    )\n",
        "    valid_dataloader = dgl.dataloading.DataLoader(\n",
        "        graph, valid_nids, sampler,\n",
        "        device=device,\n",
        "        use_ddp=False,\n",
        "        batch_size=1024,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "        num_workers=0,\n",
        "    )\n",
        "    \n",
        "    model = Model(num_features, 128, num_classes).to(device)\n",
        "    # Wrap the model with distributed data parallel module.\n",
        "    if device == torch.device('cpu'):\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=None, output_device=None)\n",
        "    else:\n",
        "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[device], output_device=device)\n",
        "    \n",
        "    # Define optimizer\n",
        "    opt = torch.optim.Adam(model.parameters())\n",
        "    \n",
        "    best_accuracy = 0\n",
        "    best_model_path = './model.pt'\n",
        "    \n",
        "    # Copied from previous tutorial with changes highlighted.\n",
        "    for epoch in range(10):\n",
        "        model.train()\n",
        "\n",
        "        with tqdm.tqdm(train_dataloader) as tq:\n",
        "            for step, (input_nodes, output_nodes, mfgs) in enumerate(tq):\n",
        "                # feature copy from CPU to GPU takes place here\n",
        "                inputs = mfgs[0].srcdata['feat']\n",
        "                labels = mfgs[-1].dstdata['label']\n",
        "\n",
        "                predictions = model(mfgs, inputs)\n",
        "\n",
        "                loss = F.cross_entropy(predictions, labels)\n",
        "                opt.zero_grad()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "                accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
        "\n",
        "                tq.set_postfix({'loss': '%.03f' % loss.item(), 'acc': '%.03f' % accuracy}, refresh=False)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Evaluate on only the first GPU.\n",
        "        if proc_id == 0:\n",
        "            predictions = []\n",
        "            labels = []\n",
        "            with tqdm.tqdm(valid_dataloader) as tq, torch.no_grad():\n",
        "                for input_nodes, output_nodes, mfgs in tq:\n",
        "                    inputs = mfgs[0].srcdata['feat']\n",
        "                    labels.append(mfgs[-1].dstdata['label'].cpu().numpy())\n",
        "                    predictions.append(model(mfgs, inputs).argmax(1).cpu().numpy())\n",
        "                predictions = np.concatenate(predictions)\n",
        "                labels = np.concatenate(labels)\n",
        "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
        "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
        "                if best_accuracy < accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    torch.save(model.state_dict(), best_model_path)\n",
        "\n",
        "        # Note that this tutorial does not train the whole model to the end.\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Spawning Trainer Processes\n",
        "--------------------------\n",
        "\n",
        "A typical scenario for multi-GPU training with DDP is to replicate the\n",
        "model once per GPU, and spawn one trainer process per GPU.\n",
        "\n",
        "Normally, DGL maintains only one sparse matrix representation (usually COO)\n",
        "for each graph, and will create new formats when some APIs are called for\n",
        "efficiency.  For instance, calling ``in_degrees`` will create a CSC\n",
        "representation for the graph, and calling ``out_degrees`` will create a\n",
        "CSR representation.  A consequence is that if a graph is shared to\n",
        "trainer processes via copy-on-write *before* having its CSC/CSR\n",
        "created, each trainer will create its own CSC/CSR replica once ``in_degrees``\n",
        "or ``out_degrees`` is called.  To avoid this, you need to create\n",
        "all sparse matrix representations beforehand using the ``create_formats_``\n",
        "method:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph.create_formats_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then you can spawn the subprocesses to train with multiple GPUs.\n",
        "\n",
        "\n",
        ".. code:: python\n",
        "\n",
        "   # Say you have four GPUs.\n",
        "   if __name__ == '__main__':\n",
        "       num_gpus = 4\n",
        "       import torch.multiprocessing as mp\n",
        "       mp.spawn(run, args=(list(range(num_gpus)),), nprocs=num_gpus)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Thumbnail credits: Stanford CS224W Notes\n",
        "# sphinx_gallery_thumbnail_path = '_static/blitz_1_introduction.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
            "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
            "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
            "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n"
          ]
        },
        {
          "ename": "ProcessExitedException",
          "evalue": "process 0 terminated with exit code 1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_gpus\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:246\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    240\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m start_method\n\u001b[1;32m    244\u001b[0m     )\n\u001b[1;32m    245\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:202\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/multiprocessing/spawn.py:153\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    146\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with signal \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, name),\n\u001b[1;32m    147\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m             signal_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ProcessExitedException(\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocess \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with exit code \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (error_index, exitcode),\n\u001b[1;32m    155\u001b[0m             error_index\u001b[38;5;241m=\u001b[39merror_index,\n\u001b[1;32m    156\u001b[0m             error_pid\u001b[38;5;241m=\u001b[39mfailed_process\u001b[38;5;241m.\u001b[39mpid,\n\u001b[1;32m    157\u001b[0m             exit_code\u001b[38;5;241m=\u001b[39mexitcode,\n\u001b[1;32m    158\u001b[0m         )\n\u001b[1;32m    160\u001b[0m original_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_queues[error_index]\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    161\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n",
            "\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 1"
          ]
        }
      ],
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    num_gpus = 2\n",
        "    import torch.multiprocessing as mp\n",
        "    mp.spawn(run, args=(list(range(num_gpus)),), nprocs=num_gpus)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
